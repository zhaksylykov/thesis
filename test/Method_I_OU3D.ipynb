{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-23T17:27:57.490078Z",
     "start_time": "2025-07-23T17:27:57.462776700Z"
    }
   },
   "id": "8785115d0a7a0add"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [5], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes', 'Heston', 'OrnsteinUhlenbeck'], 'dataset_id': [None], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((10, 'tanh'), (10, 'tanh'))], 'readout_nn': [((10, 'tanh'), (10, 'tanh'))], 'enc_nn': [((10, 'tanh'), (10, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['Heston'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-Heston-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((20, 'tanh'), (20, 'tanh'))], 'readout_nn': [((20, 'tanh'), (20, 'tanh'))], 'enc_nn': [((20, 'tanh'), (20, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['Heston'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-Heston-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((40, 'tanh'), (40, 'tanh'))], 'readout_nn': [((40, 'tanh'), (40, 'tanh'))], 'enc_nn': [((40, 'tanh'), (40, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['Heston'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-Heston-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((80, 'tanh'), (80, 'tanh'))], 'readout_nn': [((80, 'tanh'), (80, 'tanh'))], 'enc_nn': [((80, 'tanh'), (80, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['Heston'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-Heston-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((160, 'tanh'), (160, 'tanh'))], 'readout_nn': [((160, 'tanh'), (160, 'tanh'))], 'enc_nn': [((160, 'tanh'), (160, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['Heston'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-Heston-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((320, 'tanh'), (320, 'tanh'))], 'readout_nn': [((320, 'tanh'), (320, 'tanh'))], 'enc_nn': [((320, 'tanh'), (320, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['Heston'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-Heston-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((10, 'tanh'), (10, 'tanh'))], 'readout_nn': [((10, 'tanh'), (10, 'tanh'))], 'enc_nn': [((10, 'tanh'), (10, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-BS-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((20, 'tanh'), (20, 'tanh'))], 'readout_nn': [((20, 'tanh'), (20, 'tanh'))], 'enc_nn': [((20, 'tanh'), (20, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-BS-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((40, 'tanh'), (40, 'tanh'))], 'readout_nn': [((40, 'tanh'), (40, 'tanh'))], 'enc_nn': [((40, 'tanh'), (40, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-BS-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((80, 'tanh'), (80, 'tanh'))], 'readout_nn': [((80, 'tanh'), (80, 'tanh'))], 'enc_nn': [((80, 'tanh'), (80, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-BS-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((160, 'tanh'), (160, 'tanh'))], 'readout_nn': [((160, 'tanh'), (160, 'tanh'))], 'enc_nn': [((160, 'tanh'), (160, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-BS-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((320, 'tanh'), (320, 'tanh'))], 'readout_nn': [((320, 'tanh'), (320, 'tanh'))], 'enc_nn': [((320, 'tanh'), (320, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-BS-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((10, 'tanh'), (10, 'tanh'))], 'readout_nn': [((10, 'tanh'), (10, 'tanh'))], 'enc_nn': [((10, 'tanh'), (10, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['OrnsteinUhlenbeck'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-OU-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((20, 'tanh'), (20, 'tanh'))], 'readout_nn': [((20, 'tanh'), (20, 'tanh'))], 'enc_nn': [((20, 'tanh'), (20, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['OrnsteinUhlenbeck'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-OU-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((40, 'tanh'), (40, 'tanh'))], 'readout_nn': [((40, 'tanh'), (40, 'tanh'))], 'enc_nn': [((40, 'tanh'), (40, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['OrnsteinUhlenbeck'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-OU-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((80, 'tanh'), (80, 'tanh'))], 'readout_nn': [((80, 'tanh'), (80, 'tanh'))], 'enc_nn': [((80, 'tanh'), (80, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['OrnsteinUhlenbeck'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-OU-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((160, 'tanh'), (160, 'tanh'))], 'readout_nn': [((160, 'tanh'), (160, 'tanh'))], 'enc_nn': [((160, 'tanh'), (160, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['OrnsteinUhlenbeck'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-OU-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [10], 'learning_rate': [0.001], 'test_size': [0.2], 'training_size': [200, 400, 800, 1600, 3200, 6400, 12800], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((320, 'tanh'), (320, 'tanh'))], 'readout_nn': [((320, 'tanh'), (320, 'tanh'))], 'enc_nn': [((320, 'tanh'), (320, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['OrnsteinUhlenbeck'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'saved_models_path': ['../data/conv-study-OU-saved_models/'], 'evaluate': [True]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['HestonWOFeller'], 'dataset_id': [1], 'plot': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_HestonWOFeller/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'), (100, 'tanh'))], 'readout_nn': [((100, 'tanh'), (100, 'tanh'))], 'enc_nn': [((100, 'tanh'), (100, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['combined_OrnsteinUhlenbeck_BlackScholes'], 'plot': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_combined_OU_BS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((400, 'tanh'), (400, 'tanh'))], 'readout_nn': [((400, 'tanh'), (400, 'tanh'))], 'enc_nn': [((400, 'tanh'), (400, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['sine_BlackScholes'], 'dataset_id': [1], 'plot': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_sine_BS/']}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [None], 'readout_nn': [None], 'enc_nn': [None], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes', 'Heston', 'OrnsteinUhlenbeck'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'evaluate': [True], 'other_model': ['GRU_ODE_Bayes'], 'saved_models_path': ['../data/saved_models_GRUODEBayes-comparison/'], 'GRU_ODE_Bayes-impute': [True, False], 'GRU_ODE_Bayes-logvar': [True, False], 'GRU_ODE_Bayes-mixing': [0.0001, 0.5]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [20], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes', 'Heston', 'OrnsteinUhlenbeck'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_GRUODEBayes-comparison/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['climate'], 'data_index': [0, 1, 2, 3, 4], 'delta_t': [0.1], 'saved_models_path': ['../data/saved_models_NJODE1-climate/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((400, 'tanh'), (400, 'tanh'))], 'readout_nn': [((400, 'tanh'), (400, 'tanh'))], 'enc_nn': [((400, 'tanh'), (400, 'tanh'))], 'use_rnn': [False], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['climate'], 'data_index': [0, 1, 2, 3, 4], 'delta_t': [0.1], 'saved_models_path': ['../data/saved_models_NJODE1-climate/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [50], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.2], 'ode_nn': [None], 'readout_nn': [None], 'enc_nn': [None], 'use_rnn': [False], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [0, 1, 2, 3, 4], 'dataset': ['climate'], 'data_index': [1], 'delta_t': [0.1], 'other_model': ['GRU_ODE_Bayes'], 'GRU_ODE_Bayes-impute': [False], 'GRU_ODE_Bayes-logvar': [True], 'GRU_ODE_Bayes-mixing': [0.0001], 'GRU_ODE_Bayes-p_hidden': [25], 'GRU_ODE_Bayes-prep_hidden': [10], 'GRU_ODE_Bayes-cov_hidden': [50], 'saved_models_path': ['../data/saved_models_NJODE1-climate/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [175], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'hidden_size': [41], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['physionet'], 'quantization': [0.016], 'n_samples': [8000], 'saved_models_path': ['../data/saved_models_NJODE1_physionet_comparison/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [175], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'hidden_size': [41], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [False], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['physionet'], 'quantization': [0.016], 'n_samples': [8000], 'saved_models_path': ['../data/saved_models_NJODE1_physionet_comparison/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [50], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.01], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [False, True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [2], 'use_sig_for_classifier': [False], 'data_dict': ['LOB_dict1', 'LOB_dict2', 'LOB_dict3'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'output_midprice_only': [False], 'use_eval_on_train': [False], 'classifier_nn': [((50, 'tanh'),)], 'classifier_loss_weight': [1.0], 'saved_models_path': ['../data/saved_models_LOB/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [50], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.01], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [False, True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'use_sig_for_classifier': [True], 'data_dict': ['LOB_dict1', 'LOB_dict2', 'LOB_dict3'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'output_midprice_only': [False], 'use_eval_on_train': [False], 'classifier_nn': [((50, 'tanh'),)], 'classifier_loss_weight': [1.0], 'saved_models_path': ['../data/saved_models_LOB/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [50], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.01], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [False], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'use_sig_for_classifier': [False], 'data_dict': ['LOB_dict3'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'output_midprice_only': [False, True], 'use_eval_on_train': [False], 'residual_enc_dec': [True], 'classifier_nn': [None, ((50, 'tanh'),)], 'classifier_loss_weight': [1.0], 'saved_models_path': ['../data/saved_models_LOB/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'dataset': ['retrain_LOB'], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_retrain_LOB/'], 'load_model_id': [17, 18, 9, 10, 11, 12, 19, 20, 21, 22], 'load_saved_models_path': ['../data/saved_models_LOB/'], 'load_model_load_best': [True, False], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [1000], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'dataset': ['retrain_LOB'], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_retrain_LOB/'], 'load_model_id': [17, 18, 9, 10, 11, 12, 19, 20, 21, 22], 'load_saved_models_path': ['../data/saved_models_LOB/'], 'load_model_load_best': [True, False], 'new_classifier_nn': [{'nn_desc': ((200, 'tanh'), (200, 'tanh')), 'dropout_rate': 0.1, 'bias': True}], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [1000], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'dataset': ['retrain_LOB'], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_retrain_LOB/'], 'load_model_id': [17, 18, 9, 10, 11, 12, 19, 20, 21, 22], 'load_saved_models_path': ['../data/saved_models_LOB/'], 'load_model_load_best': [True, False], 'new_classifier_nn': [{'nn_desc': ((200, 'tanh'), (200, 'tanh'), (200, 'tanh'), (200, 'tanh')), 'dropout_rate': 0.1, 'bias': True}], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [50], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.01], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [False], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'use_sig_for_classifier': [False], 'data_dict': ['LOB_dict_K_3', 'LOB_dict_K_6'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'output_midprice_only': [False], 'use_eval_on_train': [False], 'residual_enc_dec': [True], 'classifier_nn': [((50, 'tanh'),)], 'classifier_loss_weight': [1.0], 'saved_models_path': ['../data/saved_models_LOB_K/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'dataset': ['retrain_LOB'], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_retrain_LOB_K/'], 'load_model_id': [1, 2], 'load_saved_models_path': ['../data/saved_models_LOB_K/'], 'load_model_load_best': [True, False], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [1000], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'dataset': ['retrain_LOB'], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_retrain_LOB_K/'], 'load_model_id': [1, 2], 'load_saved_models_path': ['../data/saved_models_LOB_K/'], 'load_model_load_best': [True, False], 'new_classifier_nn': [{'nn_desc': ((200, 'tanh'), (200, 'tanh')), 'dropout_rate': 0.1, 'bias': True}], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [1000], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'dataset': ['retrain_LOB'], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_retrain_LOB_K/'], 'load_model_id': [1, 2], 'load_saved_models_path': ['../data/saved_models_LOB_K/'], 'load_model_load_best': [True, False], 'new_classifier_nn': [{'nn_desc': ((200, 'tanh'), (200, 'tanh'), (200, 'tanh'), (200, 'tanh')), 'dropout_rate': 0.1, 'bias': True}], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [50], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.01], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [False], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'use_sig_for_classifier': [False], 'data_dict': ['LOB_dict3_2', 'LOB_dict_K_3_2', 'LOB_dict_K_6_2'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'output_midprice_only': [False], 'use_eval_on_train': [False], 'residual_enc_dec': [True], 'classifier_nn': [((50, 'tanh'),)], 'classifier_loss_weight': [1.0], 'saved_models_path': ['../data/saved_models_LOB_n/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'dataset': ['retrain_LOB'], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_retrain_LOB_n/'], 'load_model_id': [1, 2, 3], 'load_saved_models_path': ['../data/saved_models_LOB_n/'], 'load_model_load_best': [True, False], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [1000], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'dataset': ['retrain_LOB'], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_retrain_LOB_n/'], 'load_model_id': [1, 2, 3], 'load_saved_models_path': ['../data/saved_models_LOB_n/'], 'load_model_load_best': [True, False], 'new_classifier_nn': [{'nn_desc': ((200, 'tanh'), (200, 'tanh')), 'dropout_rate': 0.1, 'bias': True}], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [1000], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'dataset': ['retrain_LOB'], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_retrain_LOB_n/'], 'load_model_id': [1, 2, 3], 'load_saved_models_path': ['../data/saved_models_LOB_n/'], 'load_model_load_best': [True, False], 'new_classifier_nn': [{'nn_desc': ((200, 'tanh'), (200, 'tanh'), (200, 'tanh'), (200, 'tanh')), 'dropout_rate': 0.1, 'bias': True}], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [None], 'enc_nn': [((50, 'tanh'),)], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BM', 'BMandVar'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'train_readout_only': [True], 'saved_models_path': ['../data/saved_models_randNJODE/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BM', 'BMandVar'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'other_model': ['randomizedNJODE'], 'saved_models_path': ['../data/saved_models_randNJODE/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['DoublePendulum'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'other_model': ['randomizedNJODE'], 'saved_models_path': ['../data/saved_models_randNJODE/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'data_dict': ['BlackScholes_dict'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [False, True], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'other_model': ['randomizedNJODE'], 'saved_models_path': ['../data/saved_models_randNJODE_BS/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.01, 0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [None], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [None], 'readout_nn': [None], 'enc_nn': [((100, 'tanh'),)], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [True, False], 'input_sig': [True, False], 'level': [2], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'other_model': ['NJmodel'], 'saved_models_path': ['../data/saved_models_NJmodel/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [None, ((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [True, False], 'input_sig': [True, False], 'level': [2], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_NJmodel/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [None], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [None], 'readout_nn': [None], 'enc_nn': [((2000, 'tanh'),), ((200, 'tanh'), (200, 'tanh'))], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BlackScholes'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [True], 'input_sig': [True], 'level': [2], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'other_model': ['NJmodel'], 'saved_models_path': ['../data/saved_models_NJmodel/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'data_dict': ['BS_dep_obs_dict'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [True], 'input_sig': [True], 'level': [3], 'masked': [False], 'residual_dec': [True, False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'plot_obs_prob': [True], 'saved_models_path': ['../data/saved_models_DepObservations/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),), None], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True], 'residual_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['BM_NoisyObs_dict'], 'which_loss': ['easy', 'noisy_obs'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BMNoisyObs/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),), None], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['BM_NoisyObs_dict'], 'which_loss': ['easy', 'noisy_obs'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BMNoisyObs/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'dataset': ['physionet'], 'dataset_id': [None], 'which_loss': ['easy', 'noisy_obs'], 'quantization': [0.016], 'n_samples': [8000], 'saved_models_path': ['../data/saved_models_PhysioNet_NJODE3/'], 'obs_noise': [{'std_factor': 0.0, 'seed': 333}, {'std_factor': 0.2, 'seed': 333}, {'std_factor': 0.4, 'seed': 333}, {'std_factor': 0.6, 'seed': 333}, {'std_factor': 0.8, 'seed': 333}, {'std_factor': 1.0, 'seed': 333}]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [False, True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [2], 'dataset': ['climate'], 'data_index': [0, 1, 2, 3, 4], 'which_loss': ['easy', 'noisy_obs'], 'delta_t': [0.1], 'saved_models_path': ['../data/saved_models_Climate/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'),)], 'readout_nn': [((200, 'tanh'),)], 'enc_nn': [((200, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False], 'level': [3], 'data_dict': ['DP_dict3'], 'test_data_dict': ['DP_dict3_test'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_ODEexp_DoublePendulum/'], 'use_current_y_for_ode': [False, True], 'use_observation_as_input': [False, True, 0.5, 0.75, 0.25, 'lambda x: np.random.random(1) < 1-x/200', 'lambda x: np.random.random(1) < 1-x/100'], 'val_use_observation_as_input': [False], 'eval_use_true_paths': [True], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [300], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'),)], 'readout_nn': [((200, 'tanh'),)], 'enc_nn': [((200, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False], 'level': [3], 'data_dict': ['DP_dict4'], 'test_data_dict': ['DP_dict3_test'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_ODEexp_DoublePendulum/'], 'use_current_y_for_ode': [True], 'use_observation_as_input': ['lambda x: np.random.random() < 1-x/100'], 'val_use_observation_as_input': [False], 'eval_use_true_paths': [True], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [300], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((400, 'tanh'),)], 'readout_nn': [((400, 'tanh'),)], 'enc_nn': [((400, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False], 'level': [3], 'data_dict': ['DP_dict4'], 'test_data_dict': ['DP_dict3_test'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_ODEexp_DoublePendulum/'], 'use_current_y_for_ode': [True], 'use_observation_as_input': ['lambda x: np.random.random() < 1-x/100'], 'val_use_observation_as_input': [False], 'eval_use_true_paths': [True], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [300], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False], 'level': [3], 'data_dict': ['DP_dict4'], 'test_data_dict': ['DP_dict3_test'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_ODEexp_DoublePendulum/'], 'use_current_y_for_ode': [True], 'use_observation_as_input': ['lambda x: np.random.random() < 1-x/100'], 'val_use_observation_as_input': [False], 'eval_use_true_paths': [True], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [300], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'relu'), (200, 'tanh'))], 'readout_nn': [((200, 'relu'), (200, 'tanh'))], 'enc_nn': [((200, 'relu'), (200, 'tanh'))], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False], 'level': [3], 'data_dict': ['DP_dict4'], 'test_data_dict': ['DP_dict3_test'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_ODEexp_DoublePendulum/'], 'use_current_y_for_ode': [True], 'use_observation_as_input': ['lambda x: np.random.random() < 1-x/100'], 'val_use_observation_as_input': [False], 'eval_use_true_paths': [True], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['BS_LT_dict'], 'test_data_dict': ['BS_LT_dict_test'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_BS_LongTerm/'], 'use_current_y_for_ode': [False, True], 'use_observation_as_input': [True, 'lambda x: np.random.random(1) < 1-x/100'], 'val_use_observation_as_input': [False], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['BS_LT_dict2'], 'test_data_dict': ['BS_LT_dict2_test'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_BS_LongTerm/'], 'use_current_y_for_ode': [False, True], 'use_observation_as_input': [True, 'lambda x: np.random.random(1) < 1-x/100'], 'val_use_observation_as_input': [False], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['BS_LT_dict1'], 'test_data_dict': ['BS_LT_dict_test'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_BS_LongTerm/'], 'use_current_y_for_ode': [False, True], 'use_observation_as_input': [True, 'lambda x: np.random.random(1) < 1-x/100'], 'val_use_observation_as_input': [False], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [175], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'dataset': ['physionet'], 'dataset_id': [None], 'which_loss': ['easy'], 'quantization': [0.016], 'n_samples': [8000], 'saved_models_path': ['../data/saved_models_LongTerm_physionet/'], 'use_current_y_for_ode': [False, True], 'use_observation_as_input': ['lambda x: np.random.random(1) < 1-x/350', 'lambda x: np.random.random(1) < 1-(x-100)/150', 'lambda x: np.random.random(1) < 1-min(x-75,50)/200'], 'val_use_observation_as_input': [True]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),), None], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_BSUP_dict1'], 'test_data_dict': ['PF_BSUP_dict1_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),), None], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_BSUP_dict1'], 'test_data_dict': ['PF_BSUP_dict1_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),), None], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_BSUP_dict2'], 'test_data_dict': ['PF_BSUP_dict2_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams2/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),), None], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_BSUP_dict2'], 'test_data_dict': ['PF_BSUP_dict2_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams2/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS1_1'], 'test_data_dict': ['PF_BSUP_dict_CS1_test_1'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS1_2'], 'test_data_dict': ['PF_BSUP_dict_CS1_test_2'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS1_3'], 'test_data_dict': ['PF_BSUP_dict_CS1_test_3'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS1_4'], 'test_data_dict': ['PF_BSUP_dict_CS1_test_4'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS1_5'], 'test_data_dict': ['PF_BSUP_dict_CS1_test_5'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS1_6'], 'test_data_dict': ['PF_BSUP_dict_CS1_test_6'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS2_1'], 'test_data_dict': ['PF_BSUP_dict_CS2_test_1'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS2_2'], 'test_data_dict': ['PF_BSUP_dict_CS2_test_2'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS2_3'], 'test_data_dict': ['PF_BSUP_dict_CS2_test_3'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS2_4'], 'test_data_dict': ['PF_BSUP_dict_CS2_test_4'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS2_5'], 'test_data_dict': ['PF_BSUP_dict_CS2_test_5'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['PF_BSUP_dict_CS2_6'], 'test_data_dict': ['PF_BSUP_dict_CS2_test_6'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BSUncertainParams_CS/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100, 200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True], 'func_appl_X': [['power-2']], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_BMwUD_dict1'], 'test_data_dict': ['PF_BMwUD_dict1_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_variance': [True], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BMwUncertainDrift/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100, 200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['IO_BM_Filter_dict_1'], 'test_data_dict': ['IO_BM_Filter_dict_1_test'], 'which_loss': ['very_easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_variance': [False], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_IO_BMFilter/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['IO_BS_dict'], 'test_data_dict': ['IO_BS_dict_test'], 'which_loss': ['IO', 'easy'], 'which_val_loss': ['jump'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BS_learn_jumps/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'residual_enc_dec': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [3], 'data_dict': ['IO_BS_dict'], 'test_data_dict': ['IO_BS_dict_test'], 'which_loss': ['IO', 'easy'], 'which_val_loss': ['jump'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BS_learn_jumps/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_CIR_dict1'], 'test_data_dict': ['PF_CIR_dict1_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_CIRUncertainParams/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_CIR_dict1'], 'test_data_dict': ['PF_CIR_dict1_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_CIRUncertainParams/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_CIR_dict2'], 'test_data_dict': ['PF_CIR_dict2_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_CIRUncertainParams/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_CIR_dict2'], 'test_data_dict': ['PF_CIR_dict2_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_CIRUncertainParams/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_CIR_dict3'], 'test_data_dict': ['PF_CIR_dict3_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_CIRUncertainParams/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_CIR_dict3'], 'test_data_dict': ['PF_CIR_dict3_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_CIRUncertainParams/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_CIR_dict4'], 'test_data_dict': ['PF_CIR_dict4_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_CIRUncertainParams/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['PF_CIR_dict4'], 'test_data_dict': ['PF_CIR_dict4_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_CIRUncertainParams/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),)], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['IO_BMClass_dict'], 'test_data_dict': ['IO_BMClass_dict_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BMClassification/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),)], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True, False], 'residual_enc_dec': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['IO_BMClass_dict'], 'test_data_dict': ['IO_BMClass_dict_test'], 'which_loss': ['IO'], 'coord_wise_tau': [False], 'use_y_for_ode': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [False], 'use_cond_exp': [True], 'saved_models_path': ['../data/saved_models_BMClassification/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'),)], 'readout_nn': [((200, 'tanh'),)], 'enc_nn': [((200, 'tanh'),)], 'use_rnn': [True, False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [3], 'residual_enc_dec': [False, True], 'data_dict': ['BM_Quantiles'], 'test_data_dict': ['BM_Quantiles_test'], 'which_loss': ['quantile_jump', 'quantile'], 'loss_quantiles': [[0.1, 0.5, 0.9]], 'which_val_loss': ['quantile'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_BMandQuantiles/'], 'use_current_y_for_ode': [False], 'use_y_for_ode': [False], 'use_observation_as_input': [True, 'lambda x: np.random.random(1) < 1-x/200'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [5], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'data_dict': ['poisson_pp_dict'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_PPP/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10, 50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'data_dict': ['FBM_1_dict'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [50, 200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10, 50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2, 3], 'data_dict': ['FBM_1_dict'], 'which_loss': ['standard', 'easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [50, 200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10, 50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2, 3], 'data_dict': ['FBM_1_dict'], 'which_loss': ['standard', 'easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh')), None], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['FBM_1_dict'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False], 'level': [3], 'data_dict': ['FBM_1_dict'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [1, 2, 4, 5, 6, 7, 8, 9, 10], 'data_dict': ['FBM_1_dict'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [1, 2, 4, 5, 6, 7, 8, 9, 10], 'data_dict': ['FBM_1_dict'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),), None], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'dataset': ['BM2DCorr'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'masked': [True], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BM2D/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),), None], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'dataset': ['BM2DCorr'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'masked': [True], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BM2D/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [None], 'enc_nn': [((50, 'tanh'),)], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BMandVar'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BMandVar/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [None], 'enc_nn': [((50, 'tanh'),)], 'func_appl_X': [['power-2']], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BM'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [False], 'masked': [False], 'plot': [True], 'plot_variance': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BMandVar/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'data_dict': ['BS_dep_intensity_dict'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_DepIntensity/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'data_dict': ['BS_dep_intensity_dict'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [True], 'input_sig': [True], 'level': [3], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_DepIntensity/']}\n",
      "param_dict: {'epochs': [175], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'dataset': ['physionet'], 'dataset_id': [None], 'which_loss': ['easy'], 'quantization': [0.016], 'n_samples': [8000], 'saved_models_path': ['../data/saved_models_PhysioNet/']}\n",
      "param_dict: {'epochs': [175], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'dataset': ['physionet'], 'dataset_id': [None], 'which_loss': ['easy'], 'quantization': [0.016], 'n_samples': [8000], 'saved_models_path': ['../data/saved_models_PhysioNet/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [2], 'dataset': ['climate'], 'data_index': [0, 1, 2, 3, 4], 'which_loss': ['easy'], 'delta_t': [0.1], 'saved_models_path': ['../data/saved_models_Climate/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [2], 'dataset': ['climate'], 'data_index': [0, 1, 2, 3, 4], 'which_loss': ['easy'], 'delta_t': [0.1], 'saved_models_path': ['../data/saved_models_Climate/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100, 200, 400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'),)], 'readout_nn': [((200, 'tanh'),)], 'enc_nn': [((200, 'tanh'),)], 'use_rnn': [False, True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [3], 'data_dict': ['DP_dict1', 'DP_dict2'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_DoublePendulum/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100, 200, 400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'relu'),)], 'readout_nn': [((200, 'relu'),)], 'enc_nn': [((200, 'relu'),)], 'use_rnn': [False, True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [3], 'data_dict': ['DP_dict1', 'DP_dict2'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_DoublePendulum/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'),)], 'readout_nn': [((200, 'tanh'),)], 'enc_nn': [((200, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False], 'level': [3], 'data_dict': ['DP_dict3'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_DoublePendulum/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),), None], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'data_dict': ['BM_Filter_dict_1'], 'test_data_dict': ['BM_Filter_dict_testdata'], 'which_loss': ['easy'], 'use_y_for_ode': [True], 'masked': [True], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BMFiltering/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'relu'),)], 'readout_nn': [((200, 'relu'),), None], 'enc_nn': [((200, 'relu'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [2], 'data_dict': ['BM_TimeLag_dict_1', 'BM_TimeLag_dict_2'], 'test_data_dict': ['BM_TimeLag_dict_testdata'], 'which_loss': ['easy'], 'residual_enc_dec': [False], 'ode_input_scaling_func': ['tanh'], 'enc_input_t': [True], 'coord_wise_tau': [True], 'use_y_for_ode': [True], 'masked': [True], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BMTimeLag/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'which_loss': ['easy'], 'dataset': ['OrnsteinUhlenbeckMultiDimensional'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_OrnsteinUhlenbeckMultiDimensional/']}\n",
      "param_dict: {'epochs': [100], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'which_loss': ['easy_vol'], 'dataset': ['OrnsteinUhlenbeckForZ'], 'dataset_id': [None], 'use_cond_exp': [False], 'eval_use_true_paths': [True], 'plot': [True], 'paths_to_plot': [(0,)], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_OrnsteinUhlenbeckForZ/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [5], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'data_dict': ['poisson_pp_dict'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_PPP/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10, 50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'data_dict': ['FBM_1_dict'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [50, 200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10, 50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2, 3], 'data_dict': ['FBM_1_dict'], 'which_loss': ['standard', 'easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [50, 200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10, 50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2, 3], 'data_dict': ['FBM_1_dict'], 'which_loss': ['standard', 'easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh')), None], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [3], 'data_dict': ['FBM_1_dict'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False], 'level': [3], 'data_dict': ['FBM_1_dict'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [1, 2, 4, 5, 6, 7, 8, 9, 10], 'data_dict': ['FBM_1_dict'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [1, 2, 4, 5, 6, 7, 8, 9, 10], 'data_dict': ['FBM_1_dict'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_FBM/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),), None], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'dataset': ['BM2DCorr'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'masked': [True], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BM2D/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'relu'),)], 'readout_nn': [((100, 'relu'),), None], 'enc_nn': [((100, 'relu'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'dataset': ['BM2DCorr'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'masked': [True], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BM2D/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [None], 'enc_nn': [((50, 'tanh'),)], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BMandVar'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BMandVar/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [None], 'enc_nn': [((50, 'tanh'),)], 'func_appl_X': [['power-2']], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'dataset': ['BM'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [False], 'masked': [False], 'plot': [True], 'plot_variance': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BMandVar/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'data_dict': ['BS_dep_intensity_dict'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [False], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_DepIntensity/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'data_dict': ['BS_dep_intensity_dict'], 'dataset_id': [None], 'which_loss': ['easy'], 'coord_wise_tau': [False], 'use_y_for_ode': [True], 'use_rnn': [True], 'input_sig': [True], 'level': [3], 'masked': [False], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_DepIntensity/']}\n",
      "param_dict: {'epochs': [175], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'dataset': ['physionet'], 'dataset_id': [None], 'which_loss': ['easy'], 'quantization': [0.016], 'n_samples': [8000], 'saved_models_path': ['../data/saved_models_PhysioNet/']}\n",
      "param_dict: {'epochs': [175], 'batch_size': [50], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'hidden_size': [50], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'dataset': ['physionet'], 'dataset_id': [None], 'which_loss': ['easy'], 'quantization': [0.016], 'n_samples': [8000], 'saved_models_path': ['../data/saved_models_PhysioNet/']}\n",
      "param_dict: {'epochs': [200], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'),)], 'readout_nn': [((50, 'tanh'),)], 'enc_nn': [((50, 'tanh'),)], 'use_rnn': [True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [2], 'dataset': ['climate'], 'data_index': [0, 1, 2, 3, 4], 'which_loss': ['easy'], 'delta_t': [0.1], 'saved_models_path': ['../data/saved_models_Climate/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'), (200, 'tanh'))], 'readout_nn': [((200, 'tanh'), (200, 'tanh'))], 'enc_nn': [((200, 'tanh'), (200, 'tanh'))], 'use_rnn': [True], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [2], 'dataset': ['climate'], 'data_index': [0, 1, 2, 3, 4], 'which_loss': ['easy'], 'delta_t': [0.1], 'saved_models_path': ['../data/saved_models_Climate/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100, 200, 400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'),)], 'readout_nn': [((200, 'tanh'),)], 'enc_nn': [((200, 'tanh'),)], 'use_rnn': [False, True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [3], 'data_dict': ['DP_dict1', 'DP_dict2'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_DoublePendulum/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [50, 100, 200, 400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'relu'),)], 'readout_nn': [((200, 'relu'),)], 'enc_nn': [((200, 'relu'),)], 'use_rnn': [False, True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False, True], 'level': [3], 'data_dict': ['DP_dict1', 'DP_dict2'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_DoublePendulum/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'tanh'),)], 'readout_nn': [((200, 'tanh'),)], 'enc_nn': [((200, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [False], 'level': [3], 'data_dict': ['DP_dict3'], 'which_loss': ['easy'], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'saved_models_path': ['../data/saved_models_DoublePendulum/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [200], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((100, 'tanh'),)], 'readout_nn': [((100, 'tanh'),), None], 'enc_nn': [((100, 'tanh'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True], 'level': [2], 'data_dict': ['BM_Filter_dict_1'], 'test_data_dict': ['BM_Filter_dict_testdata'], 'which_loss': ['easy'], 'use_y_for_ode': [True], 'masked': [True], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BMFiltering/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [200], 'batch_size': [200], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [400], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((200, 'relu'),)], 'readout_nn': [((200, 'relu'),), None], 'enc_nn': [((200, 'relu'),)], 'use_rnn': [True], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'input_sig': [True, False], 'level': [2], 'data_dict': ['BM_TimeLag_dict_1', 'BM_TimeLag_dict_2'], 'test_data_dict': ['BM_TimeLag_dict_testdata'], 'which_loss': ['easy'], 'residual_enc_dec': [False], 'ode_input_scaling_func': ['tanh'], 'enc_input_t': [True], 'coord_wise_tau': [True], 'use_y_for_ode': [True], 'masked': [True], 'plot': [True], 'evaluate': [True], 'paths_to_plot': [(0, 1, 2, 3, 4)], 'plot_same_yaxis': [True], 'saved_models_path': ['../data/saved_models_BMTimeLag/'], 'dataset_id': [1]}\n",
      "param_dict: {'epochs': [100], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'which_loss': ['easy'], 'dataset': ['OrnsteinUhlenbeckMultiDimensional'], 'dataset_id': [None], 'plot': [True], 'paths_to_plot': [(0,)], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_OrnsteinUhlenbeckMultiDimensional/']}\n",
      "param_dict: {'epochs': [100], 'batch_size': [100], 'save_every': [1], 'learning_rate': [0.001], 'test_size': [0.2], 'seed': [398], 'hidden_size': [10], 'bias': [True], 'dropout_rate': [0.1], 'ode_nn': [((50, 'tanh'), (50, 'tanh'))], 'readout_nn': [((50, 'tanh'), (50, 'tanh'))], 'enc_nn': [((50, 'tanh'), (50, 'tanh'))], 'use_rnn': [False], 'func_appl_X': [[]], 'solver': ['euler'], 'weight': [0.5], 'weight_decay': [1.0], 'which_loss': ['easy_vol'], 'dataset': ['OrnsteinUhlenbeckForZ'], 'dataset_id': [None], 'use_cond_exp': [False], 'eval_use_true_paths': [True], 'plot': [True], 'paths_to_plot': [(0,)], 'evaluate': [True], 'saved_models_path': ['../data/saved_models_OrnsteinUhlenbeckForZ/']}\n",
      "mob30009889\n",
      "SERVER=False\n"
     ]
    }
   ],
   "source": [
    "import NJODE.data_utils as data_utils \n",
    "import NJODE.train as train "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-18T10:19:22.754061600Z",
     "start_time": "2025-07-18T10:19:22.431442600Z"
    }
   },
   "id": "e11ff26febd87ea"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "dataset_dic = {\n",
    "    'mean': np.array([1.2, 1.0, 1.5]).tolist(),\n",
    "    'volatility': np.array([[0.2, 0.1, 0.1], [0.1, 0.25, 0.1], [0.1,0.1, 0.3]]).tolist(),\n",
    "    'speed': np.array([[0.3, 0.0, 0.0], [0.0, 0.3, 0.0],[0.0,0.0,0.3]]).tolist(),\n",
    "    'nb_paths': 10000,\n",
    "    'nb_steps': 100,\n",
    "    'S0': np.array([1.0, 1.5 ,2.0]).tolist(),\n",
    "    'maturity': 1.0,\n",
    "    'dimension': 3,\n",
    "    'obs_perc': 1.0,\n",
    "    'scheme': 'euler',\n",
    "    'return_vol': False,\n",
    "    'v0': 1,\n",
    "    'hurst': 0.75,\n",
    "    'FBMmethod': 'daviesharte',\n",
    "    'model_name': 'OrnsteinUhlenbeckWithCrossTerms',\n",
    "    'dt': 0.01\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-18T10:19:36.536502300Z",
     "start_time": "2025-07-18T10:19:36.512890200Z"
    }
   },
   "id": "2c50689ba6ecafbe"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "datasetpath, dataset_id = data_utils.create_dataset(\"OrnsteinUhlenbeckWithCrossTerms\",dataset_dic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-18T10:19:45.638432600Z",
     "start_time": "2025-07-18T10:19:37.251947Z"
    }
   },
   "id": "ca5e17744b2f7502"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "94"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-18T10:19:45.655398300Z",
     "start_time": "2025-07-18T10:19:45.638432600Z"
    }
   },
   "id": "74d3baf192da1262"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "dataset_metadata = data_utils.load_metadata(stock_model_name=\"OrnsteinUhlenbeckWithCrossTerms\",time_id=94)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-18T10:19:53.071104900Z",
     "start_time": "2025-07-18T10:19:53.018720500Z"
    }
   },
   "id": "95802a04eeb50eda"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_metadata[\"dimension\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-18T10:19:53.654987Z",
     "start_time": "2025-07-18T10:19:53.603358Z"
    }
   },
   "id": "60b7c9dbea78f12e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FBMmethod': 'daviesharte', 'S0': [1.0, 1.5, 2.0], 'dimension': 3, 'dt': 0.01, 'hurst': 0.75, 'maturity': 1.0, 'mean': [1.2, 1.0, 1.5], 'model_name': 'OrnsteinUhlenbeckWithCrossTerms', 'nb_paths': 10000, 'nb_steps': 100, 'obs_perc': 1.0, 'return_vol': False, 'scheme': 'euler', 'speed': [[0.3, 0.0, 0.0], [0.0, 0.3, 0.0], [0.0, 0.0, 0.3]], 'v0': 1, 'volatility': [[0.2, 0.1, 0.1], [0.1, 0.25, 0.1], [0.1, 0.1, 0.3]]}\n",
      "using loss: easy\n",
      "neuralODE use input scaling with tanh\n",
      "use residual network: input_size=9, output_size=10\n",
      "use residual network: input_size=10, output_size=9\n",
      "model-id: None\n",
      "\n",
      "using CPU\n",
      "input_coords: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "output_coords: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "input_size: 9\n",
      "output_size: 9\n",
      "signature_coords: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "new model_id=87\n",
      "model params:\n",
      "{\"batch_size\": 100, \"bias\": true, \"data_dict\": {\"FBMmethod\": \"daviesharte\", \"S0\": [1.0, 1.5, 2.0], \"dimension\": 3, \"dt\": 0.01, \"hurst\": 0.75, \"maturity\": 1.0, \"mean\": [1.2, 1.0, 1.5], \"model_name\": \"OrnsteinUhlenbeckWithCrossTerms\", \"nb_paths\": 10000, \"nb_steps\": 100, \"obs_perc\": 1.0, \"return_vol\": false, \"scheme\": \"euler\", \"speed\": [[0.3, 0.0, 0.0], [0.0, 0.3, 0.0], [0.0, 0.0, 0.3]], \"v0\": 1, \"volatility\": [[0.2, 0.1, 0.1], [0.1, 0.25, 0.1], [0.1, 0.1, 0.3]]}, \"dataset\": \"OrnsteinUhlenbeckWithCrossTerms\", \"dataset_id\": 94, \"dropout_rate\": 0.1, \"enc_nn\": [[50, \"tanh\"], [50, \"tanh\"]], \"epochs\": 100, \"hidden_size\": 10, \"input_size\": 9, \"learning_rate\": 0.001, \"ode_nn\": [[50, \"tanh\"], [50, \"tanh\"]], \"optimal_val_loss\": NaN, \"options\": {\"input_coords\": [0, 1, 2, 3, 4, 5, 6, 7, 8], \"input_sig\": false, \"level\": 2, \"output_coords\": [0, 1, 2, 3, 4, 5, 6, 7, 8], \"use_cond_exp\": false, \"which_loss\": \"easy\"}, \"output_size\": 9, \"readout_nn\": [[50, \"tanh\"], [50, \"tanh\"]], \"seed\": 398, \"solver\": \"euler\", \"test_size\": 0.2, \"use_rnn\": false, \"weight\": 0.5, \"weight_decay\": 1.0}\n",
      "initiate new model ...\n",
      "\n",
      "model overview:\n",
      "NJODE(\n",
      "  (ode_f): ODEFunc(\n",
      "    (f): Sequential(\n",
      "      (0): Linear(in_features=21, out_features=50, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "      (4): Tanh()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder_map): FFNN(\n",
      "    (ffnn): Sequential(\n",
      "      (0): Linear(in_features=9, out_features=50, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "      (4): Tanh()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (readout_map): FFNN(\n",
      "    (ffnn): Sequential(\n",
      "      (0): Linear(in_features=10, out_features=50, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "      (4): Tanh()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=9, bias=True)\n",
      "    )\n",
      "  )\n",
      ") \n",
      "\n",
      "# parameters=11279\n",
      "\n",
      "# trainable parameters=11279\n",
      "\n",
      "start training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:14,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 1, weight=0.50000, train-loss=0.59640, optimal-val-loss=nan, val-loss=0.07065, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:987: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n",
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.07065\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: inf, new-best-loss: 0.07065, epoch: 1\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 2, weight=0.50000, train-loss=0.21742, optimal-val-loss=nan, val-loss=0.05935, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.05935\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.07065, new-best-loss: 0.05935, epoch: 2\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:17,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 3, weight=0.50000, train-loss=0.12772, optimal-val-loss=nan, val-loss=0.05540, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.05540\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.05935, new-best-loss: 0.05540, epoch: 3\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:17,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 4, weight=0.50000, train-loss=0.09795, optimal-val-loss=nan, val-loss=0.05327, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.05327\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.05540, new-best-loss: 0.05327, epoch: 4\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 5, weight=0.50000, train-loss=0.07837, optimal-val-loss=nan, val-loss=0.05199, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.05199\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.05327, new-best-loss: 0.05199, epoch: 5\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:17,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 6, weight=0.50000, train-loss=0.07349, optimal-val-loss=nan, val-loss=0.05110, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.05110\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.05199, new-best-loss: 0.05110, epoch: 6\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 7, weight=0.50000, train-loss=0.06799, optimal-val-loss=nan, val-loss=0.05048, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.05048\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.05110, new-best-loss: 0.05048, epoch: 7\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 8, weight=0.50000, train-loss=0.06374, optimal-val-loss=nan, val-loss=0.05005, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.05005\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.05048, new-best-loss: 0.05005, epoch: 8\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:17,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 9, weight=0.50000, train-loss=0.06503, optimal-val-loss=nan, val-loss=0.04969, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04969\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.05005, new-best-loss: 0.04969, epoch: 9\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:17,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 10, weight=0.50000, train-loss=0.05896, optimal-val-loss=nan, val-loss=0.04946, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04946\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04969, new-best-loss: 0.04946, epoch: 10\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 11, weight=0.50000, train-loss=0.06335, optimal-val-loss=nan, val-loss=0.04948, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04948\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:17,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 12, weight=0.50000, train-loss=0.05820, optimal-val-loss=nan, val-loss=0.04924, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04924\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04946, new-best-loss: 0.04924, epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:17,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 13, weight=0.50000, train-loss=0.05817, optimal-val-loss=nan, val-loss=0.04901, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04901\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04924, new-best-loss: 0.04901, epoch: 13\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 14, weight=0.50000, train-loss=0.05854, optimal-val-loss=nan, val-loss=0.04890, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04890\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04901, new-best-loss: 0.04890, epoch: 14\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:17,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 15, weight=0.50000, train-loss=0.05820, optimal-val-loss=nan, val-loss=0.04886, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04886\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04890, new-best-loss: 0.04886, epoch: 15\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:17,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 16, weight=0.50000, train-loss=0.05668, optimal-val-loss=nan, val-loss=0.04876, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04876\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04886, new-best-loss: 0.04876, epoch: 16\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 17, weight=0.50000, train-loss=0.05549, optimal-val-loss=nan, val-loss=0.04871, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04871\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04876, new-best-loss: 0.04871, epoch: 17\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 18, weight=0.50000, train-loss=0.05630, optimal-val-loss=nan, val-loss=0.04871, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04871\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:17,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 19, weight=0.50000, train-loss=0.05608, optimal-val-loss=nan, val-loss=0.04858, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04858\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04871, new-best-loss: 0.04858, epoch: 19\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 20, weight=0.50000, train-loss=0.05333, optimal-val-loss=nan, val-loss=0.04862, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04862\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 21, weight=0.50000, train-loss=0.05355, optimal-val-loss=nan, val-loss=0.04846, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04846\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04858, new-best-loss: 0.04846, epoch: 21\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 22, weight=0.50000, train-loss=0.05218, optimal-val-loss=nan, val-loss=0.04844, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04844\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04846, new-best-loss: 0.04844, epoch: 22\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 23, weight=0.50000, train-loss=0.05357, optimal-val-loss=nan, val-loss=0.04850, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04850\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 24, weight=0.50000, train-loss=0.05283, optimal-val-loss=nan, val-loss=0.04847, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04847\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 25, weight=0.50000, train-loss=0.05104, optimal-val-loss=nan, val-loss=0.04881, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04881\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 26, weight=0.50000, train-loss=0.05243, optimal-val-loss=nan, val-loss=0.04826, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04826\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04844, new-best-loss: 0.04826, epoch: 26\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:20,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 27, weight=0.50000, train-loss=0.05269, optimal-val-loss=nan, val-loss=0.04858, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04858\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:20,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 28, weight=0.50000, train-loss=0.05063, optimal-val-loss=nan, val-loss=0.04827, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04827\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 29, weight=0.50000, train-loss=0.05143, optimal-val-loss=nan, val-loss=0.04857, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04857\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 30, weight=0.50000, train-loss=0.05117, optimal-val-loss=nan, val-loss=0.04823, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04823\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04826, new-best-loss: 0.04823, epoch: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:20,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 31, weight=0.50000, train-loss=0.05058, optimal-val-loss=nan, val-loss=0.04836, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04836\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 32, weight=0.50000, train-loss=0.05100, optimal-val-loss=nan, val-loss=0.04830, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04830\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 33, weight=0.50000, train-loss=0.05324, optimal-val-loss=nan, val-loss=0.04823, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04823\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04823, new-best-loss: 0.04823, epoch: 33\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 34, weight=0.50000, train-loss=0.05144, optimal-val-loss=nan, val-loss=0.04840, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04840\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 35, weight=0.50000, train-loss=0.05053, optimal-val-loss=nan, val-loss=0.04818, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04818\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04823, new-best-loss: 0.04818, epoch: 35\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 36, weight=0.50000, train-loss=0.04890, optimal-val-loss=nan, val-loss=0.04839, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04839\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 37, weight=0.50000, train-loss=0.04912, optimal-val-loss=nan, val-loss=0.04825, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04825\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 38, weight=0.50000, train-loss=0.05131, optimal-val-loss=nan, val-loss=0.04799, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04799\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04818, new-best-loss: 0.04799, epoch: 38\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:26,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 39, weight=0.50000, train-loss=0.05181, optimal-val-loss=nan, val-loss=0.04841, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04841\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 40, weight=0.50000, train-loss=0.05197, optimal-val-loss=nan, val-loss=0.04847, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04847\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 41, weight=0.50000, train-loss=0.04978, optimal-val-loss=nan, val-loss=0.04790, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04790\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04799, new-best-loss: 0.04790, epoch: 41\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 42, weight=0.50000, train-loss=0.05080, optimal-val-loss=nan, val-loss=0.04846, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04846\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 43, weight=0.50000, train-loss=0.05039, optimal-val-loss=nan, val-loss=0.04804, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04804\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 44, weight=0.50000, train-loss=0.04973, optimal-val-loss=nan, val-loss=0.04809, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04809\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:21,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 45, weight=0.50000, train-loss=0.05081, optimal-val-loss=nan, val-loss=0.04827, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04827\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:18,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 46, weight=0.50000, train-loss=0.04845, optimal-val-loss=nan, val-loss=0.04800, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04800\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:22,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 47, weight=0.50000, train-loss=0.05073, optimal-val-loss=nan, val-loss=0.04801, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04801\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 48, weight=0.50000, train-loss=0.04987, optimal-val-loss=nan, val-loss=0.04806, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04806\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:19,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 49, weight=0.50000, train-loss=0.05041, optimal-val-loss=nan, val-loss=0.04820, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04820\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:22,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 50, weight=0.50000, train-loss=0.04888, optimal-val-loss=nan, val-loss=0.04800, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04800\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:21,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 51, weight=0.50000, train-loss=0.04903, optimal-val-loss=nan, val-loss=0.04809, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04809\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:24,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 52, weight=0.50000, train-loss=0.04868, optimal-val-loss=nan, val-loss=0.04788, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04788\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04790, new-best-loss: 0.04788, epoch: 52\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:24,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 53, weight=0.50000, train-loss=0.04911, optimal-val-loss=nan, val-loss=0.04821, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04821\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:26,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 54, weight=0.50000, train-loss=0.04894, optimal-val-loss=nan, val-loss=0.04788, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04788\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:27,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 55, weight=0.50000, train-loss=0.04778, optimal-val-loss=nan, val-loss=0.04802, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04802\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:28,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 56, weight=0.50000, train-loss=0.04864, optimal-val-loss=nan, val-loss=0.04807, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04807\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:29,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 57, weight=0.50000, train-loss=0.04813, optimal-val-loss=nan, val-loss=0.04780, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04780\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04788, new-best-loss: 0.04780, epoch: 57\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:34,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 58, weight=0.50000, train-loss=0.04832, optimal-val-loss=nan, val-loss=0.04785, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04785\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:33,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 59, weight=0.50000, train-loss=0.04719, optimal-val-loss=nan, val-loss=0.04792, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04792\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:33,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 60, weight=0.50000, train-loss=0.04726, optimal-val-loss=nan, val-loss=0.04797, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04797\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:35,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 61, weight=0.50000, train-loss=0.04825, optimal-val-loss=nan, val-loss=0.04796, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04796\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:33,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 62, weight=0.50000, train-loss=0.04930, optimal-val-loss=nan, val-loss=0.04788, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04788\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:37,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 63, weight=0.50000, train-loss=0.04744, optimal-val-loss=nan, val-loss=0.04791, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04791\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:39,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 64, weight=0.50000, train-loss=0.04752, optimal-val-loss=nan, val-loss=0.04793, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04793\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:38,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 65, weight=0.50000, train-loss=0.04814, optimal-val-loss=nan, val-loss=0.04776, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04776\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04780, new-best-loss: 0.04776, epoch: 65\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:59,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 66, weight=0.50000, train-loss=0.04765, optimal-val-loss=nan, val-loss=0.04787, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04787\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:40,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 67, weight=0.50000, train-loss=0.04842, optimal-val-loss=nan, val-loss=0.04790, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04790\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:44,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 68, weight=0.50000, train-loss=0.04752, optimal-val-loss=nan, val-loss=0.04777, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04777\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:47,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 69, weight=0.50000, train-loss=0.04842, optimal-val-loss=nan, val-loss=0.04779, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04779\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:48,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 70, weight=0.50000, train-loss=0.04869, optimal-val-loss=nan, val-loss=0.04776, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04776\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04776, new-best-loss: 0.04776, epoch: 70\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:52,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 71, weight=0.50000, train-loss=0.04550, optimal-val-loss=nan, val-loss=0.04785, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04785\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:48,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 72, weight=0.50000, train-loss=0.04914, optimal-val-loss=nan, val-loss=0.04773, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04773\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04776, new-best-loss: 0.04773, epoch: 72\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:50,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 73, weight=0.50000, train-loss=0.04998, optimal-val-loss=nan, val-loss=0.04778, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04778\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:51,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 74, weight=0.50000, train-loss=0.04701, optimal-val-loss=nan, val-loss=0.04779, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04779\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:52,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 75, weight=0.50000, train-loss=0.04744, optimal-val-loss=nan, val-loss=0.04773, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04773\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04773, new-best-loss: 0.04773, epoch: 75\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:51,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 76, weight=0.50000, train-loss=0.04747, optimal-val-loss=nan, val-loss=0.04776, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04776\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:52,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 77, weight=0.50000, train-loss=0.04805, optimal-val-loss=nan, val-loss=0.04777, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04777\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:52,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 78, weight=0.50000, train-loss=0.04727, optimal-val-loss=nan, val-loss=0.04776, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04776\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:57,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 79, weight=0.50000, train-loss=0.04799, optimal-val-loss=nan, val-loss=0.04774, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04774\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:54,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 80, weight=0.50000, train-loss=0.04751, optimal-val-loss=nan, val-loss=0.04775, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04775\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:59,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 81, weight=0.50000, train-loss=0.04653, optimal-val-loss=nan, val-loss=0.04773, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04773\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [01:05,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 82, weight=0.50000, train-loss=0.04882, optimal-val-loss=nan, val-loss=0.04769, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04769\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04773, new-best-loss: 0.04769, epoch: 82\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:56,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 83, weight=0.50000, train-loss=0.04713, optimal-val-loss=nan, val-loss=0.04772, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04772\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:51,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 84, weight=0.50000, train-loss=0.04766, optimal-val-loss=nan, val-loss=0.04766, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04766\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04769, new-best-loss: 0.04766, epoch: 84\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:47,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 85, weight=0.50000, train-loss=0.04714, optimal-val-loss=nan, val-loss=0.04771, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04771\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:45,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 86, weight=0.50000, train-loss=0.04836, optimal-val-loss=nan, val-loss=0.04771, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04771\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:43,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 87, weight=0.50000, train-loss=0.04764, optimal-val-loss=nan, val-loss=0.04763, \n",
      "plotting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg868\\thesis\\NJODE\\train.py:998: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metric = pd.concat([df_metric, df_m_app], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04763\n",
      "save model ...\n",
      "saved!\n",
      "save new best model: last-best-loss: 0.04766, new-best-loss: 0.04763, epoch: 87\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:41,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 88, weight=0.50000, train-loss=0.04677, optimal-val-loss=nan, val-loss=0.04766, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04766\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:39,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 89, weight=0.50000, train-loss=0.04786, optimal-val-loss=nan, val-loss=0.04773, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04773\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:38,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 90, weight=0.50000, train-loss=0.04721, optimal-val-loss=nan, val-loss=0.04768, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04768\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:36,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 91, weight=0.50000, train-loss=0.04741, optimal-val-loss=nan, val-loss=0.04770, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04770\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:37,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 92, weight=0.50000, train-loss=0.04709, optimal-val-loss=nan, val-loss=0.04768, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04768\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:32,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 93, weight=0.50000, train-loss=0.04759, optimal-val-loss=nan, val-loss=0.04768, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04768\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:32,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 94, weight=0.50000, train-loss=0.04607, optimal-val-loss=nan, val-loss=0.04792, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04792\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:29,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 95, weight=0.50000, train-loss=0.04820, optimal-val-loss=nan, val-loss=0.04768, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04768\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:30,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 96, weight=0.50000, train-loss=0.04945, optimal-val-loss=nan, val-loss=0.04765, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04765\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:29,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 97, weight=0.50000, train-loss=0.04798, optimal-val-loss=nan, val-loss=0.04766, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04766\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:30,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 98, weight=0.50000, train-loss=0.04798, optimal-val-loss=nan, val-loss=0.04769, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04769\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:29,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 99, weight=0.50000, train-loss=0.04615, optimal-val-loss=nan, val-loss=0.04774, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04774\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:29,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ...\n",
      "epoch 100, weight=0.50000, train-loss=0.04831, optimal-val-loss=nan, val-loss=0.04772, \n",
      "plotting ...\n",
      "optimal test-loss (with current weight=0.50000): 0.00000\n",
      "model test-loss (with current weight=0.50000): 0.04772\n",
      "save model ...\n",
      "saved!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.train(dataset=\"OrnsteinUhlenbeckWithCrossTerms\", dataset_id=94, data_dict=dataset_dic, use_cond_exp = False, input_coords=[0,1,2,3,4,5,6,7,8], output_coords=[0,1,2,3,4,5,6,7,8], which_loss=\"easy\", input_sig=False, level=2 )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-18T11:17:23.022901300Z",
     "start_time": "2025-07-18T10:20:15.817156200Z"
    }
   },
   "id": "417c27c0337c652d"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss: easy\n",
      "neuralODE use input scaling with tanh\n",
      "use residual network: input_size=9, output_size=10\n",
      "use residual network: input_size=10, output_size=9\n"
     ]
    },
    {
     "data": {
      "text/plain": "NJODE(\n  (ode_f): ODEFunc(\n    (f): Sequential(\n      (0): Linear(in_features=21, out_features=50, bias=True)\n      (1): Tanh()\n      (2): Dropout(p=0, inplace=False)\n      (3): Linear(in_features=50, out_features=50, bias=True)\n      (4): Tanh()\n      (5): Dropout(p=0, inplace=False)\n      (6): Linear(in_features=50, out_features=10, bias=True)\n    )\n  )\n  (encoder_map): FFNN(\n    (ffnn): Sequential(\n      (0): Linear(in_features=9, out_features=50, bias=True)\n      (1): Tanh()\n      (2): Dropout(p=0, inplace=False)\n      (3): Linear(in_features=50, out_features=50, bias=True)\n      (4): Tanh()\n      (5): Dropout(p=0, inplace=False)\n      (6): Linear(in_features=50, out_features=10, bias=True)\n    )\n  )\n  (readout_map): FFNN(\n    (ffnn): Sequential(\n      (0): Linear(in_features=10, out_features=50, bias=True)\n      (1): Tanh()\n      (2): Dropout(p=0, inplace=False)\n      (3): Linear(in_features=50, out_features=50, bias=True)\n      (4): Tanh()\n      (5): Dropout(p=0, inplace=False)\n      (6): Linear(in_features=50, out_features=9, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_dict = {\n",
    "    'input_size': 9,\n",
    "    'hidden_size': 10,\n",
    "    'output_size': 9,\n",
    "    'ode_nn': ((50, \"tanh\"), (50, \"tanh\")),\n",
    "    'readout_nn': ((50, \"tanh\"), (50, \"tanh\")),\n",
    "    'enc_nn': ((50, \"tanh\"), (50, \"tanh\")),\n",
    "    'use_rnn': False,\n",
    "    'options': {'which_loss': 'easy'},\n",
    "    \"input_coords\": np.arange(9),\n",
    "    \"output_coords\": np.arange(9),\n",
    "    \"signature_coords\": np.arange(9)\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from models import get_ckpt_model, NJODE\n",
    "model = NJODE(**params_dict).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "get_ckpt_model(\"../data/saved_models/id-87/best_checkpoint/\", model, optimizer, device)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-23T17:28:15.208685200Z",
     "start_time": "2025-07-23T17:28:15.148349900Z"
    }
   },
   "id": "fdac2b5e77f72b3d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "data_1k=np.load(\"../data/training_data/OrnsteinUhlenbeckWithCrossTerms-92/data.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-23T17:28:19.219455200Z",
     "start_time": "2025-07-23T17:28:19.164103500Z"
    }
   },
   "id": "c7e184088881d3d3"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(1000, 9, 101)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1k.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-17T15:18:28.038353600Z",
     "start_time": "2025-07-17T15:18:28.003730800Z"
    }
   },
   "id": "4268b29891e12684"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.  , 1.5 , 2.  , 1.  , 2.25, 4.  , 1.5 , 2.  , 3.  ])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1k[0,:,0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-17T15:19:31.007186100Z",
     "start_time": "2025-07-17T15:19:30.976478300Z"
    }
   },
   "id": "89f85b6727af7469"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "start_X = torch.tensor(np.array([1.0, 1.5, 2.0, 1.0, 2.25, 4.0, 1.5, 2.0, 3.0]), dtype=torch.float).unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-17T15:20:03.339278Z",
     "start_time": "2025-07-17T15:20:03.307896100Z"
    }
   },
   "id": "5dd487f86a5124a7"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 9])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-17T15:20:11.275419Z",
     "start_time": "2025-07-17T15:20:11.234667700Z"
    }
   },
   "id": "56e174a2e56fb7b6"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def vector_to_symmetric_matrix(X):\n",
    "    \"\"\"\n",
    "    Converts input vector X of shape (batch_size, dim*2 + dim*(dim - 1)//2)\n",
    "    to a symmetric matrix of shape (batch_size, dim, dim).\n",
    "    \"\"\"\n",
    "    batch_size, total_dim = X.shape\n",
    "\n",
    "    # Solve quadratic equation: dim^2 + dim = 2 * total_dim\n",
    "    # to find dim\n",
    "    a = 1\n",
    "    b = 1\n",
    "    c = -2 * total_dim\n",
    "    discriminant = b**2 - 4*a*c\n",
    "    dim = int((-b + discriminant**0.5) / (2 * a))\n",
    "\n",
    "    assert dim * 2 + dim * (dim - 1) // 2 == total_dim, \"Invalid input dimensions\"\n",
    "\n",
    "    # Extract parts\n",
    "    squared_terms = X[:, dim:2*dim]  # shape: (batch_size, dim)\n",
    "    cross_terms = X[:, 2*dim:]       # shape: (batch_size, dim*(dim - 1)//2)\n",
    "\n",
    "    symm_matrix = torch.zeros(batch_size, dim, dim, dtype=X.dtype, device=X.device)\n",
    "\n",
    "    # Fill diagonal\n",
    "    for i in range(dim):\n",
    "        symm_matrix[:, i, i] = squared_terms[:, i]\n",
    "\n",
    "    # Fill off-diagonal with cross terms\n",
    "    idx = 0\n",
    "    for i in range(dim):\n",
    "        for j in range(i+1, dim):\n",
    "            symm_matrix[:, i, j] = cross_terms[:, idx]\n",
    "            symm_matrix[:, j, i] = cross_terms[:, idx]\n",
    "            idx += 1\n",
    "\n",
    "    return symm_matrix\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-18T11:19:05.941446900Z",
     "start_time": "2025-07-18T11:19:05.924072600Z"
    }
   },
   "id": "70cf14f1fae92db6"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    " def generate_next_value( X_t, mu_t, sigma_t, delta_t):\n",
    "    \"\"\"\n",
    "    Generate the next value in the time series using the Euler-Maruyama scheme.\n",
    "\n",
    "    :param X_t: current value tensor of shape (batch_size, d)\n",
    "    :param mu_t: drift coefficient tensor of shape (batch_size, d)\n",
    "    :param sigma_t: diffusion coefficient tensor of shape ( d, d)\n",
    "    :param delta_t: time difference float\n",
    "    :return: next value tensor of shape (batch_size, d)\n",
    "    \"\"\"\n",
    "\n",
    "    delta_Wt = torch.randn_like(X_t) * np.sqrt(delta_t)\n",
    "    delta_Wt_sigma_t = torch.bmm(delta_Wt.unsqueeze(1), sigma_t).squeeze(1)\n",
    "    X_t_next = X_t + mu_t * delta_t + delta_Wt_sigma_t\n",
    "\n",
    "    return X_t_next"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-23T17:28:25.565870700Z",
     "start_time": "2025-07-23T17:28:25.543738500Z"
    }
   },
   "id": "21ae09574c52d923"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def expand_features(X):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X: Tensor of shape (batch_size, dim)\n",
    "    Output:\n",
    "        Y: Tensor of shape (batch_size, dim + dim + dim*(dim - 1)//2)\n",
    "           = (batch_size, dim*2 + dim*(dim - 1)//2)\n",
    "    \"\"\"\n",
    "    batch_size, dim = X.shape\n",
    "\n",
    "    squares = X ** 2  # shape: (batch_size, dim)\n",
    "\n",
    "    # Compute pairwise products (only upper triangle without diagonal)\n",
    "    products = []\n",
    "    for i in range(dim):\n",
    "        for j in range(i + 1, dim):\n",
    "            products.append(X[:, i] * X[:, j])  # shape: (batch_size,)\n",
    "\n",
    "    products = torch.stack(products, dim=1) if products else torch.empty(batch_size, 0, device=X.device)\n",
    "\n",
    "    # Concatenate all parts\n",
    "    Y = torch.cat([X, squares, products], dim=1)\n",
    "\n",
    "    return Y\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-23T17:28:31.870226300Z",
     "start_time": "2025-07-23T17:28:31.859227600Z"
    }
   },
   "id": "6d0708aafbf95c9c"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.0000, 1.5000, 2.0000, 1.0000, 2.2500, 4.0000, 1.5000, 2.0000, 3.0000]])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_features(start_X[:,:3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-17T16:26:43.689458800Z",
     "start_time": "2025-07-17T16:26:43.577399200Z"
    }
   },
   "id": "c9ec4d9f1960497c"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0002, 1.4997, 1.9997, 1.0004, 2.2492, 3.9987, 1.4999, 2.0003, 2.9990]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.0205, -0.0347, -0.0345]], grad_fn=<DivBackward0>)\n",
      "tensor([[[ 0.0002, -0.0038,  0.0224],\n",
      "         [-0.0038,  0.0235,  0.0171],\n",
      "         [ 0.0224,  0.0171,  0.0088]]], grad_fn=<SubBackward0>)\n",
      "[[-0.0226716   0.01728697  0.03791685]]\n",
      "[[0.00152589 0.21351068 0.15108797]]\n",
      "tensor([[0.9876, 1.5127, 1.9993, 0.9754, 2.2885, 3.9973, 1.4939, 1.9747, 3.0245]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.0203, -0.0358, -0.0343]], grad_fn=<DivBackward0>)\n",
      "tensor([[[-2.5041, -0.6056, -2.5337],\n",
      "         [-0.6056,  3.9580,  2.5756],\n",
      "         [-2.5337,  2.5756, -0.1330]]], grad_fn=<SubBackward0>)\n",
      "[[-4.223031   -0.02017581  5.564077  ]]\n",
      "[[1.87609384e-03 2.82269409e+00 1.31107389e+00]]\n",
      "tensor([[0.9777, 1.5416, 1.9990, 0.9560, 2.3769, 3.9962, 1.5073, 1.9547, 3.0820]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.0202, -0.0371, -0.0339]], grad_fn=<DivBackward0>)\n",
      "tensor([[[-4.4416,  0.7318, -4.5343],\n",
      "         [ 0.7318, 12.8049,  8.3227],\n",
      "         [-4.5343,  8.3227, -0.2480]]], grad_fn=<SubBackward0>)\n",
      "[[-8.802767   -0.02276397 16.940775  ]]\n",
      "[[1.78765632e-03 4.63517201e+00 2.08833204e+00]]\n",
      "tensor([[0.2487, 2.0376, 1.9983, 0.0620, 4.1507, 3.9972, 0.5040, 0.4955, 4.0713]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.1081,  0.0149, -0.1007]], grad_fn=<DivBackward0>)\n",
      "tensor([[[-9.3850e+01, -9.9823e+01, -1.5064e+02],\n",
      "         [-9.9823e+01,  1.9001e+02,  1.0731e+02],\n",
      "         [-1.5064e+02,  1.0731e+02,  1.2558e-01]]], grad_fn=<SubBackward0>)\n",
      "[[ 3.0168561e+02 -2.0552167e+02  1.1765389e-01]]\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Matrix is not positive definite",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLinAlgError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(sig_hat_t)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39meigvals(sig_hat_t\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()))\n\u001B[1;32m---> 25\u001B[0m sigma_hat_t \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcholesky\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmake_positive_definite\u001B[49m\u001B[43m(\u001B[49m\u001B[43msig_hat_t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39meigvals(sigma_hat_t))\n\u001B[0;32m     27\u001B[0m X_next_points \u001B[38;5;241m=\u001B[39m generate_next_value( X_t[:,:dim], mu_hat_t, torch\u001B[38;5;241m.\u001B[39mfrom_numpy(sigma_hat_t)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat), delta_t)\n",
      "File \u001B[1;32mC:\\Program Files\\Miniconda3\\envs\\thesis\\lib\\site-packages\\numpy\\linalg\\linalg.py:779\u001B[0m, in \u001B[0;36mcholesky\u001B[1;34m(a)\u001B[0m\n\u001B[0;32m    777\u001B[0m t, result_t \u001B[38;5;241m=\u001B[39m _commonType(a)\n\u001B[0;32m    778\u001B[0m signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD->D\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m isComplexType(t) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124md->d\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 779\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43mgufunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    780\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrap(r\u001B[38;5;241m.\u001B[39mastype(result_t, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "File \u001B[1;32mC:\\Program Files\\Miniconda3\\envs\\thesis\\lib\\site-packages\\numpy\\linalg\\linalg.py:115\u001B[0m, in \u001B[0;36m_raise_linalgerror_nonposdef\u001B[1;34m(err, flag)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_raise_linalgerror_nonposdef\u001B[39m(err, flag):\n\u001B[1;32m--> 115\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LinAlgError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMatrix is not positive definite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mLinAlgError\u001B[0m: Matrix is not positive definite"
     ]
    }
   ],
   "source": [
    "dim=3\n",
    "times=np.array([])\n",
    "time_ptr = np.array([0])\n",
    "obs_idx = torch.tensor([],dtype=torch.long)\n",
    "delta_t=0.01\n",
    "X= torch.tensor([], dtype=torch.float)\n",
    "start_X = torch.tensor(np.array([1.0, 1.5, 2.0, 1.0, 2.25, 4.0, 1.5, 2.0, 3.0]), dtype=torch.float).unsqueeze(0)\n",
    "n_obs_ot=torch.tensor([0], dtype=torch.float)\n",
    "\n",
    "for i in range(1,101):\n",
    "    T=0.01*i\n",
    "    pred=model.get_pred(times, time_ptr, X, obs_idx, delta_t, T, start_X, n_obs_ot)[\"pred\"][-1] \n",
    "    print(pred)\n",
    "    if i==1:\n",
    "        X_t = start_X \n",
    "    else: \n",
    "        X_t = X_next  \n",
    "    mu_hat_t = (pred[:,:dim]-X_t[:,:dim])/delta_t \n",
    "    print(mu_hat_t)\n",
    "    term_1= (vector_to_symmetric_matrix(pred) - vector_to_symmetric_matrix(start_X))/delta_t \n",
    "    term_2 = X_t[:,:dim].T@mu_hat_t+mu_hat_t.T@X_t[:,:dim]\n",
    "    sig_hat_t = term_1-term_2\n",
    "    print(sig_hat_t)\n",
    "    print(np.linalg.eigvals(sig_hat_t.detach().numpy()))\n",
    "    sigma_hat_t = np.linalg.cholesky(make_positive_definite(sig_hat_t.detach().numpy()))\n",
    "    print(np.linalg.eigvals(sigma_hat_t))\n",
    "    X_next_points = generate_next_value( X_t[:,:dim], mu_hat_t, torch.from_numpy(sigma_hat_t).to(torch.float), delta_t)\n",
    "    X_next = expand_features(X_next_points)\n",
    "    X= torch.cat((X,X_next), dim=0)\n",
    "    times=np.append(times, delta_t*i)\n",
    "    time_ptr = np.append(time_ptr, i)\n",
    "    obs_idx=torch.cat((obs_idx, torch.tensor([0], dtype=torch.long)))\n",
    "    n_obs_ot+=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-23T17:29:26.542318500Z",
     "start_time": "2025-07-23T17:29:26.448202700Z"
    }
   },
   "id": "c803232f029ce9d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "993a2d27563904c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
